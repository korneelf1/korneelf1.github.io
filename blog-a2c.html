<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>A2C Spiking Neural Networks - Korneel Vandenberghe</title>
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
    <style>
        .blog-page {
            max-width: 800px;
            margin: 0 auto;
            padding: 2rem;
        }
        
        .blog-header {
            text-align: center;
            margin-bottom: 3rem;
            padding-bottom: 2rem;
            border-bottom: 2px solid #e9ecef;
        }
        
        .blog-title {
            font-size: 2.5rem;
            color: #2c3e50;
            margin-bottom: 1rem;
        }
        
        .blog-meta {
            color: #6c757d;
            font-size: 1.1rem;
            margin-bottom: 1rem;
        }
        
        .back-link {
            display: inline-flex;
            align-items: center;
            color: #3498db;
            text-decoration: none;
            font-weight: 500;
            margin-bottom: 2rem;
            transition: color 0.3s ease;
        }
        
        .back-link:hover {
            color: #2980b9;
        }
        
        .back-link i {
            margin-right: 0.5rem;
        }
        
        .blog-content {
            line-height: 1.8;
            font-size: 1.1rem;
        }
        
        .blog-content h2 {
            color: #2c3e50;
            margin: 2rem 0 1rem 0;
            font-size: 1.8rem;
        }
        
        .blog-content h3 {
            color: #2c3e50;
            margin: 1.5rem 0 0.8rem 0;
            font-size: 1.4rem;
        }
        
        .blog-content p {
            margin-bottom: 1.2rem;
        }
        
        .blog-content ul {
            margin: 1rem 0;
            padding-left: 1.5rem;
        }
        
        .blog-content li {
            margin-bottom: 0.5rem;
        }
        
        .blog-content strong {
            color: #2c3e50;
            font-weight: 600;
        }
        
        .blog-content em {
            background-color: #f8f9fa;
            padding: 0.2rem 0.4rem;
            border-radius: 3px;
            font-family: 'Courier New', monospace;
        }
        
        .figure-container {
            margin-bottom: 2rem;
        }
        
        .blog-figure {
            width: 100%;
            height: auto;
            margin-bottom: 0.5rem;
        }
        
        .figure-caption {
            font-size: 0.9rem;
            color: #6c757d;
        }
        
        .blog-banner {
            width: 100%;
            margin-bottom: 2rem;
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
        }
        
        .blog-banner img {
            width: 100%;
            height: auto;
            display: block;
        }
    </style>
</head>
<body>
    <nav class="navbar">
        <div class="nav-content">
            <div class="logo">Korneel Van den Berghe</div>
            <ul class="nav-links">
                <li><a href="index.html#about">About Me</a></li>
                <li><a href="index.html#projects">Projects</a></li>
                <li><a href="index.html#cv">CV</a></li>
            </ul>
        </div>
    </nav>

    <main style="margin-top: 80px;">
        <div class="blog-page">
            <a href="index.html#projects" class="back-link">
                <i class="fas fa-arrow-left"></i>
                Back to Projects
            </a>
            
            <div class="blog-header">
                <h1 class="blog-title">A Comparative Study of Spiking and Non-Spiking Neural Networks in Actor-Critic Reinforcement Learning</h1>
                <p class="blog-meta">Published: June 2024 | Reading time: 15 min | Research Project</p>
                <p class="blog-authors"><strong>Author:</strong> K. Van den Berghe, Technische Universiteit van Delft</p>
            </div>
            
            <div class="blog-banner">
                <img src="project_figures/a2c_collage.png" alt="A2C Research Overview: Noise Robustness, Surrogate Gradients, and Training Performance">
            </div>
            
            <div class="blog-content">
                <p>Within the world of autonomous micro robotics, constraints in computational resources and energy usage is a pressing issue, motivating the exploration of novel innovative algorithms to bring intelligence to these systems. This work explores the challenges and opportunities that arise when deploying spiking neural networks as workers in actor-critic deep reinforcement learning, specifically using the A2C algorithm on the CartPole task.</p>
                
                <h2>Project Overview and Motivation</h2>
                <p>Reinforcement learning (RL) has experienced significant advancements in recent years, enabling robots to achieve human-like performance on complex tasks. Actor-critic reinforcement learning algorithms have shown to be pivotal in training efficiently in challenging environments, offering potential for higher sample efficiency through parallel training.</p>
                
                <p>Concurrently, neuromorphic algorithms, such as spiking neural networks (SNN), have garnered attention in the field of deep learning. These bio-inspired algorithms have demonstrated utility in handling temporal data and offer high energy efficiency when run on specialized hardware. Combining the possibility to learn from experience through reinforcement learning with the energy-efficient characteristics of spiking neural networks holds great opportunity in the field of small mobile robotics, where computational resources are constrained.</p>
                
                <p>The primary objective was to compare the performance and computational complexity of spiking neural networks against traditional artificial neural networks in actor-critic reinforcement learning, specifically addressing:</p>
                <ul>
                    <li><strong>Training Efficiency:</strong> Comparing convergence patterns between SNN and ANN architectures</li>
                    <li><strong>Computational Complexity:</strong> Analyzing energy efficiency and operational requirements</li>
                    <li><strong>Noise Robustness:</strong> Evaluating performance under realistic sensor noise conditions</li>
                    <li><strong>Pruning Opportunities:</strong> Exploring model compression techniques unique to SNNs</li>
                </ul>
                
                <h2>Technical Implementation</h2>
                <p>The project employed a comprehensive comparison framework using the A2C (Advantage Actor-Critic) algorithm on the CartPole task:</p>
                
                <h3>A2C Reinforcement Learning</h3>
                <p>The A2C algorithm was chosen for its advantages over DQN-based approaches. Unlike DQN, which requires experience replay and batch processing, A2C uses multiple workers operating in parallel, eliminating the need for experience replay. This opens doors to learning on resource-constrained systems, where the overhead required for storing past experiences and training in batches can be detrimental to system performance.</p>
                
                <h3>CartPole Task</h3>
                <p>The CartPole task consists of balancing a pole on a cart as long as possible, receiving +1 reward for every timestep where the pole is balanced. The actor can apply either a force to the left or to the right on the cart, while observing the position and velocity of the cart and angle and angular velocity of the pole. For all models, the environment runs at a frequency of 20Hz.</p>
                
                <h3>Network Architecture and Neuron Models</h3>
                <p>To compare ANN to SNN, two models with the same architecture were used. The only difference between the models is the spiking neurons in the SNN, replacing the regular activation functions. The architecture is kept small to avoid long training times, which is an issue for SNN.</p>
                
                <p><strong>Single Layer Architecture:</strong> Input layer with 4 continuous neurons, one hidden layer with 246 neurons, and two output heads representing the actor and critic outputs.</p>
                
                <p><strong>Two Layer Architecture:</strong> Input layer with 4 neurons, two hidden layers with 128 neurons each, and two output heads.</p>
                
                <div class="figure-container">
                    <img src="projects_Latex/A2C-spiking/A Comparative Study of Spiking and Non-Spiking Neural Networks in Actor-Critic Reinforcement Learning/Figures/Training_SNN_ANN_1l246.png" alt="Training comparison between SNN and ANN architectures" class="blog-figure">
                    <p class="figure-caption"><strong>Figure 1:</strong> Training performance comparison between SNN and ANN architectures with single hidden layer (246 neurons). The SNN shows different convergence characteristics due to its spiking nature.</p>
                </div>
                
                <h3>Leaky Integrate-and-Fire (LIF) Neuron Model</h3>
                <p>The LIF neuron is a first-order model where the input current directly charges the membrane potential, which leaks over time at a rate β (the leakage parameter). When the potential exceeds a threshold, the neuron spikes and the membrane potential is reset. The dynamics are modeled by:</p>
                
                <p><em>U[t+1] = β U[t] + I_in[t+1] - R·U_thr</em></p>
                
                <p>Where R is 1 when the membrane potential exceeds the threshold and 0 otherwise.</p>
                
                <div class="figure-container">
                    <img src="projects_Latex/A2C-spiking/A Comparative Study of Spiking and Non-Spiking Neural Networks in Actor-Critic Reinforcement Learning/Figures/surrogate.png" alt="Surrogate gradient function" class="blog-figure">
                    <p class="figure-caption"><strong>Figure 2:</strong> The step function, surrogate function, and its gradient used for training spiking neural networks with backpropagation.</p>
                </div>
                
                <h3>Training with Surrogate Gradients</h3>
                <p>Due to the discrete spiking nature of neurons, traditional backpropagation cannot be directly applied. Surrogate gradients provide a solution by using a continuous, differentiable function to mimic the spiking behavior during the backward pass. The surrogate function used is the fast sigmoid function with a slope of 25.</p>
                
                <h2>Results and Performance Analysis</h2>
                <p>The research revealed several key insights about training spiking neural networks with reinforcement learning:</p>
                
                <h3>Training Characteristics</h3>
                <p>As expected, for both the deeper model and the model with only one hidden layer, the neuromorphic solution converges slower. This can partly be explained by the way the surrogate gradient works. In conventional neural networks, virtually all weights contribute to the output at each timestep, but for spiking neural networks, neurons below the threshold do not spike and therefore do not contribute to the gradient flow.</p>
                
                <div class="figure-container">
                    <img src="projects_Latex/A2C-spiking/A Comparative Study of Spiking and Non-Spiking Neural Networks in Actor-Critic Reinforcement Learning/Figures/training_3models.png" alt="Training of different SNN configurations" class="blog-figure">
                    <p class="figure-caption"><strong>Figure 3:</strong> Training of an ANN and SNN with one hidden layer of size 246. The first SNN learns to reduce β to zero (no temporal dynamics), while the second SNN has a fixed leak β=0.65.</p>
                </div>
                
                <div class="figure-container">
                    <img src="projects_Latex/A2C-spiking/A Comparative Study of Spiking and Non-Spiking Neural Networks in Actor-Critic Reinforcement Learning/Figures/Training_SNNANN2layers.png" alt="Training of two-layer architectures" class="blog-figure">
                    <p class="figure-caption"><strong>Figure 4:</strong> Training of ANN and SNN with two hidden layers of size 128. The SNN has leaks of β=0.95 for both layers.</p>
                </div>
                
                <h3>Computational Complexity Analysis</h3>
                <p>Using NeuroBench, a comprehensive comparison was made between the trained models. The results show where SNN models excel compared to their ANN equivalents:</p>
                
                <ul>
                    <li><strong>Activation Sparsity:</strong> SNNs achieve 68-92% activation sparsity compared to 0% for ANNs</li>
                    <li><strong>Synaptic Operations:</strong> SNNs require only accumulates (ACs) rather than multiply-accumulates (MACs) after spiking layers</li>
                    <li><strong>Effective Operations:</strong> Significant reduction in computational effort due to sparsity</li>
                </ul>
                
                <div class="figure-container">
                    <img src="projects_Latex/A2C-spiking/A Comparative Study of Spiking and Non-Spiking Neural Networks in Actor-Critic Reinforcement Learning/Figures/SNN_histogram1K_noLeak.png" alt="SNN neuron activity distribution" class="blog-figure">
                    <p class="figure-caption"><strong>Figure 5:</strong> Distribution of neuron activity in the trained SNN, showing the presence of dead and saturated neurons that enable pruning.</p>
                </div>
                
                <h3>Pruning Results</h3>
                <p>One of the most significant findings was the effectiveness of pruning in spiking neural networks. Dead neurons (0% spiking activity) and saturated neurons (100% spiking activity) can be removed without significant performance degradation:</p>
                
                <ul>
                    <li><strong>Single Layer SNN (β=0):</strong> Reduced from 246 to 11 neurons (95.5% reduction)</li>
                    <li><strong>Single Layer SNN (β=0.65):</strong> Reduced from 246 to 21 neurons (91.5% reduction)</li>
                    <li><strong>Performance Impact:</strong> Minimal degradation in average performance while maintaining reasonable risk levels</li>
                </ul>
                
                <p>This pruning capability is unique to spiking neural networks and provides significant advantages for deployment on resource-constrained devices.</p>
                
                <h3>Noise Robustness Analysis</h3>
                <p>An important aspect of the research was analyzing the robustness of controllers to sensor noise, which reflects real-world deployment conditions:</p>
                
                <div class="figure-container">
                    <img src="projects_Latex/A2C-spiking/A Comparative Study of Spiking and Non-Spiking Neural Networks in Actor-Critic Reinforcement Learning/Figures/Noise_Robustness_5Models.png" alt="Noise robustness comparison" class="blog-figure">
                    <p class="figure-caption"><strong>Figure 6:</strong> Noise robustness analysis of single layer models. SNNs with temporal dynamics (leaky neurons) show improved robustness at higher noise levels.</p>
                </div>
                
                <div class="figure-container">
                    <img src="projects_Latex/A2C-spiking/A Comparative Study of Spiking and Non-Spiking Neural Networks in Actor-Critic Reinforcement Learning/Figures/Noise_Robustness_2layer_models.png" alt="Noise robustness of two-layer models" class="blog-figure">
                    <p class="figure-caption"><strong>Figure 7:</strong> Noise robustness analysis of two-layer models. SNNs surpass ANN performance at noise levels above 0.04.</p>
                </div>
                
                <p>The results show that SNNs with temporal dynamics (leaky neurons) tend to be more noise robust than their non-leaky counterparts and traditional ANNs. At noise levels above 0.15, the leaky SNN models outperform their non-leaky counterparts, and at noise levels above 0.04, they surpass ANN performance.</p>
                
                <h2>Key Findings and Implications</h2>
                <p>The research revealed several important insights:</p>
                <ul>
                    <li><strong>Training Challenges:</strong> SNN training is noisier and slower than ANN training due to surrogate gradient limitations</li>
                    <li><strong>Computational Efficiency:</strong> SNNs achieve significant reductions in computational complexity through sparsity and efficient operations</li>
                    <li><strong>Pruning Potential:</strong> SNNs can be pruned by 90%+ without significant performance degradation</li>
                    <li><strong>Noise Robustness:</strong> SNNs with temporal dynamics show superior performance under noisy conditions</li>
                    <li><strong>Encoding Overhead:</strong> The encoding layer (linear layer to first spiking layer) accounts for the largest computational cost</li>
                </ul>
                
                <h2>Future Research Directions</h2>
                <p>Several promising avenues for future work have been identified:</p>
                <ul>
                    <li><strong>Efficient Encoding:</strong> Developing more efficient spiking encoders to reduce computational overhead</li>
                    <li><strong>Deeper Networks:</strong> Exploring pruning techniques for deeper SNN architectures</li>
                    <li><strong>Hardware Optimization:</strong> Leveraging specialized neuromorphic hardware for further efficiency gains</li>
                    <li><strong>Training Improvements:</strong> Developing better surrogate gradient methods for faster, more stable training</li>
                </ul>
                
                <h2>Impact and Significance</h2>
                <p>This work represents a significant contribution to the field of neuromorphic reinforcement learning, demonstrating that spiking neural networks can be effectively trained for control tasks while offering substantial advantages in computational efficiency and noise robustness.</p>
                
                <p>The findings have important implications for autonomous micro robotics, where computational resources and energy efficiency are critical constraints. The ability to achieve comparable performance with significantly reduced computational complexity makes SNNs an attractive option for deployment on resource-constrained devices.</p>
                
                <p>The research also highlights the unique advantages of spiking neural networks, particularly their ability to be aggressively pruned without performance degradation and their superior noise robustness compared to traditional artificial neural networks.</p>
            </div>
        </div>
    </main>

    <footer>
        <div class="container">
            <p>&copy; 2024 Korneel Vandenberghe. All rights reserved.</p>
        </div>
    </footer>
</body>
</html> 